## 梯度下降法：

*下文展示了学习吴恩达深度学习时较为重要的梯度下降法公式总结，有利于更好地理解梯度下降这一过程，关键在于链式法则求导*

**二元交叉熵损失函数：**

L(a, y) = - [ y * log(a) + (1 - y) * log(1 - a) ]

dL/da = - (y / a) + (1 - y) / (1 - a)

**Logistic回归的梯度输出：**


**两层神经网络梯度下降过程：（正向传播/反向传播）**

https://github.com/555yuanyuanzi/CV_base/issues/1#issuecomment-3157337164
https://github.com/555yuanyuanzi/CV_base/issues/1#issuecomment-3157336341
https://github.com/555yuanyuanzi/CV_base/issues/1#issuecomment-3157336127

**符号说明**:

- [1] 和 [2] 是上标，代表**第一层**和**第二层**。

- X: 输入矩阵，维度是 (n_x, m)，n_x是特征数，m是样本数。

- W, b: 权重矩阵和偏置向量。

- Z: 线性计算结果。

- A: 激活函数的输出结果 (A for Activation)。

- g: 激活函数，比如 ReLU 或 Sigmoid。

- dZ, dW, db: 分别代表成本函数 J 对 Z, W, b 的梯度（偏导数）。

- Y: 真实标签矩阵，维度是 (1, m)。

- *: **逐元素乘法 (element-wise product)**，不是矩阵乘法。

- .T: 矩阵的**转置 (Transpose)**。



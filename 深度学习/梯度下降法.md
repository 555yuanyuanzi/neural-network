## 梯度下降法：

*下文展示了学习吴恩达深度学习时较为重要的梯度下降法公式总结，有利于更好地理解梯度下降这一过程，关键在于链式法则求导*

**二元交叉熵损失函数：**

L(a, y) = - [ y * log(a) + (1 - y) * log(1 - a) ]

dL/da = - (y / a) + (1 - y) / (1 - a)

**Logistic回归的梯度输出：**


**两层神经网络梯度下降过程：（正向传播/反向传播）**
![Image](https://github.com/user-attachments/assets/82cf9f2c-c319-49ff-8ca1-4e27c6dd1cbe)
![Image](https://github.com/user-attachments/assets/ca9e0a65-1001-4baa-ac87-94a8f045b109)
![Image](https://github.com/user-attachments/assets/72f18f7d-b1d8-4b88-b3fc-78e64307e611)
![Image](https://github.com/user-attachments/assets/0108af37-c590-4c88-b1b0-7a285fb6554e)
**符号说明**:

- [1] 和 [2] 是上标，代表**第一层**和**第二层**。

- X: 输入矩阵，维度是 (n_x, m)，n_x是特征数，m是样本数。

- W, b: 权重矩阵和偏置向量。

- Z: 线性计算结果。

- A: 激活函数的输出结果 (A for Activation)。

- g: 激活函数，比如 ReLU 或 Sigmoid。

- dZ, dW, db: 分别代表成本函数 J 对 Z, W, b 的梯度（偏导数）。

- Y: 真实标签矩阵，维度是 (1, m)。

- *: **逐元素乘法 (element-wise product)**，不是矩阵乘法。

- .T: 矩阵的**转置 (Transpose)**。





